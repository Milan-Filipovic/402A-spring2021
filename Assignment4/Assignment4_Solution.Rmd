---
title: "Assignment 4: Sample Size and Design"
author: 'Team 1: Milan Filipovic, Vivian Truong, Lillian Chen'
date: "4/29/2021"
output: 
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# 1. Type I and II Errors, Power, Effect Size 

Type I error ($\alpha$) : probability of rejecting the null hypothesis when it is true

Type II error ($\beta$) : probability of not rejecting the null hypothesis when it is false

Power (1-$\beta$) : probability of rejecting the null hypothesis when it is false

Effect size : also known as the standardized difference $\Delta$, scales the difference in population means by the standard deviation

+ Cohen's d definitions of effect size: small = 0.2, medium = 0.5, large = 0.8.

+ should not be solely used to formulate objectives for a study for the following reasons:
  - the effect size is a function of at least three parameters, representing a substantial reduction of the parameter space
  - the experimental design may preclude estimation of some of the parameters

# 2. Basic formulas for power and sample size 

Starting with the basin sample size formula for two groups, with a two-sided alternative, normal distribution with homogeneous variances ($\sigma_0^2$ = $\sigma_1^2$ = $\sigma^2$) and equal sample sizes ($n_0$ = $n_1$ = $n$), the basic formula for sample size (Lehr's Equation) is
$$
n=\frac{16}{\Delta^2}
$$

where 
$$
\Delta = \frac{\mu_0-\mu_1}{\sigma} = \frac{\delta}{\sigma}
$$
is the treatment difference to be detected in units of the standard deviation - the standardized difference.

In the one-sample case, the numerator is 8 instead of 16. In the two-sample case, two means have to be estimated, doubling the variance and requiring two groups. Thus, the two sample scenario requires four times as many observations as the one-sample scenario.

The formula for sample size required to compare two population means $\mu_0$ and $\mu_1$ with common variance $\sigma^2$ is derived from the previous equation:
$$
n=\frac{2(z_{1-\alpha/2}+z_{1-\beta})^2}{(\frac{\mu_0-\mu_1}{\sigma})^2}
$$

Sample size plotted against power can show us the real relationship between the two to justify a sample size to an investigator. Ideally, you do not want to rely on the central limit theorem (CLT) to justify any sample size, especially since 1) 30 subjects might not be possible or feasible, or 2) the data does not allow us to use a normal approximation.


# 3. Sample size based on coefficient of variation 

Sample size formula based on coefficient of variation:

$$
n=\frac{16(CV)^2}{(ln\mu_0-ln\mu_1)^2}
$$

where CV is the coefficient of variation (CV = $\sigma_0$/$\mu_0$ = $\sigma_1$/$\mu_1$).

Specification of variation implies that the standard deviation is proportional to the mean.
This is helpful if clients do not know the standard deviation of their experiments but they might have an idea of how much their experiments or groups may vary between run to run. If there is no knowledge of the variation in a biological system, we know that CV of 30-35% is common in biological assay experiences, and a clinically significiant difference may be around 20%. We can use the numbers given from a client to then help us calculate other statistical values such as an effect size $\Delta$. Additionally, CV is often used with log normal data.

# 4. Sample size calculation for the Poisson distribution 

The Poisson distribution deals with rare events. Supposing that the means of samples from two Poisson populations are to be compared in a two-sample test, and $\theta_0$ and $\theta_1$ are the means of two populations, the required number of observations per sample is
$$
n = \frac{4}{(\sqrt\theta_0-\sqrt\theta_1)^2}
$$

which can also be rewritten as

$$
n = \frac{4}{(\theta_0+\theta_1)/2-\sqrt{\theta_0\theta_1}}
$$

Supposing that the means $\theta_0$ and $\theta_1$ are means per unit time (or unit volume) and that the observations are observed for a period of time $T$, the sample size required is
$$
n = \frac{4}{T(\sqrt\theta_0-\sqrt\theta_1)^2}
$$

The Poisson distribution is also a common model for describing radioactive scenarios. Supposing that the background rate is $\theta^*$ and the additional rates over background are $\theta_0$ and $\theta_1$, then $Y_i$ is Poisson $(\theta^*+\theta_i)$ and the sample size formula is
$$
n = \frac{4}{(\sqrt{\theta^*+\theta_0}-\sqrt{\theta^*+\theta_1})^2}
$$

# 5. Sample size calculation for the Binomial distribution 

Mean and proportions are two of the most important parameters in statistics because they arise frequently in life:

+ What proportion of patients benefit from a drug?
+ Is the C-section proportion the same for white women and black women who give birth?
+ What is the probability a person is a carrier of a rare genetic defect? 


We model $Y_i$ as a Binomial random variable with batch size $m_i$ and "success" probability $p_i$
$$
\mathbb{P}(Y_i = y_i) = \binom{m_i}{y_i} p_i^{y_i} (1 - p_i)^{m_i - y_i}.
$$
Where $E(Y_i)=np$ and $Var(Y_i)=np(1-p)$. 

Suppose we want to compare two different proportions $\pi_0$ and $\pi_1$. Recall the basic formula for calculating sample size:
$$
n=\frac{16}{\Delta^2}
$$
Belle proposes the following rule of thumb for calculating binomial sample size:
$$
n=\frac{16\bar{\pi}(1-\bar{\pi})}{(\pi_0-\pi_1)^2}
$$
Where $\bar{\pi}=\frac{(\pi_0+\pi_1)}{2}$ is used as the average variance of the two proportions. 

## Discussion and Extension

This approximation is good for values of n that come out between 10 and 100. More exact formulas should be used to determine sample sizes for values of n outside of this range. 

# 6. Systematic effects due to randomization 

Randomization refers to the act of randomly assigning subjects in a study to different treatment groups.

## Rule (Belle)
**Randomization puts systematic sources of variability into the error term.**

Care must be taken to ensure that there is no pattern or bias in the assignment of subjects and that every subject is as likely as any other to be assigned to a treatment or control group. 

## Basis for Randomization
There are three component to randomization

+ Systematic effects are turned into error. 
+ Unknown factors influencing outcome are balanced with known factors. 
+ Randomization provides the basis for statistical procedures that are used to address areas of scientific interest:
  - Tests of significance
  - Analysis of Variance 
  - Confidence Intervals


# 7. Blocking 
The concept of blocking is to break up the subjects into a number of blocks, and then to randomize the subjects within these blocks. The systematic effects between blocks can be eliminated because we do not care about these sources of variance and want minimize their impact on inference. 

## Examples of blocking
Blocks can include any group that is more homogeneous than the population. Age group or alcohol consumption could be potential blocking variables in a study interested in the effects of smoking for example, because individuals within the same age group or rate of alcohol consumption may be more homogeneous in smoking habits. 

A paired study or matched study design are strong examples of blocking because each subject is paired with an equivalent. This reduces treatment-between-subject variability. Crossover studies are also good examples of blocking because the subject receives both treatments, and so the comparison is within subject. 

## Example - Pot Position in Greenhouse
Researchers were interested in comparing different experimental designs to measure which plant growth trait resulted in the most growth. One confounding variable is position within greenhouse - some plants may be exposed to more sunlight more favorable to growth for instance. Different experimental designs were tested to determine the best way to reduce variance caused by position. One experimental design was to rotate the pots so that each plant was positioned in different parts of the greenhouse over the study. An alternative design was to block the plants according to the growth trait, and then randomize at each position in the greenhouse (fixed-position design). The rotation scheme is similar to a crossover study, whereas the fixed-position controlled for treatment-between-subject variability. The researchers found that blocking with fixed-position was more efficient than re-arrangement of pots and concluded that re-arrangement should be abandoned in favor of fixed-position design.
[Source](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0527-4)
<center>
![](images\potplant.PNG)
</center> 

# 8. Factorial designs 
Factorial design refers to the situation when more than one independent variable, or factor, is assessed simultaneously. The treatment combinations are applied to the observational unit. 

## Example 
Consider a comparative study to study the response of asthmatic subjects to two factors: pollutants (ozone vs air) and two exercise levels (active vs rest). Here we have two independent variables.Instead of conducting two separate study, using $4n$ subjects, a factorial design assigns $\frac{n}{2}$ subjects to each of the treatment combinations. 

<center>
![](images\factorial_design.PNG) 
</center>

Note that each cell in a factorial design contributes $\frac{n}{2}-1$ degrees of freedom per level, so the total degrees of freedom lost is $2n-4$. Using factorial design we can assess the following effects:

+ Main effect: The effect of the factor on the outcome separately (marginal effect).
+ Interaction effect: When the effect of one variable depends on the level of another factor. 



# 9. Balanced designs 
Balanced design refers to a study design where all treatment combinations are based on the same number of observations. In the case of an ANOVA, this refers to equal sample sizes in each group or cell. This is commonly used in factorial design.

Characteristics of balanced design include:

- Factor A & Factor B are uncorrelated
- SSA, SSAB, and SSB are orthogonal
- SSModel = SSA + SSB +SSAB
- Tests for factors and interactions are independent
- For estimation of group-level comparisons, the marginal means and least squares means are equivalent

**Rule of Thumb**: Aim for balance in the design of a study. The idea is that analyses will be more straightforward and allow additive partitioning of the total sums of squares. Additionally, balanced designs tend to minimize the standard errors of the estimates.

# 10. Missing data 

Real-world data tends to have missing data, but as statisticians we must have a plan in place to identify the type of missing data and how to adjust analyses to deal with missing data. Missing data can lead to unbalanced study design, which lead to incorrect analyses, which ultimately result in incorrect conclusions drawn from the data.

- Missing completely at random (MCAR): A variable is missing completely at random if neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing. In other words, the causes of missing data are unrelated to the data. This implies that the available data can be analyzed without worrying about bias. 

- Missing at random (MAR): Data are missing and the missingness can be explained on the basis of observed variables. A variable is missing at random if other variables in the dataset can be used to predict the missingness of the variable. In other words, the probability of missingness is the same only within groups defined by the observed data. Most modern missing data methods start from the MAR assumption. For multicenter studies, MCAR would be that one center would have more missingness than others due to something like local lab evaluation being different from other centers.

- Missing not at random (MNAR): A variable is missing not at random if the value of the unobserved variable itself predicts missingness. MNAR is the most complex case as the causes for the missingness vary for reasons unknown to the investigator.

Strategies for dealing with missing data include the following:
- omission - removing invalid data
  + listwise deletion
  + variable deletion

- imputation - placing values in place of missing data 
  + simple imputation
  + multiple imputation
  
- complete case analysis
- worst case analysis
- sensitivity analysis

If % missingness approaches levels of 10%, let alone 20%, missingness must be well thought out and the investigator should be comfortable with the method in which they handle the missingness. 

# 11. Multiple comparisons 

Multiple comparisons becomes a problem when there are multiple statistical inferences conducted at the same time. The idea is that the more inferences that are made, the more likely erroneous inferences are to occur, and failing to adjust for multiple comparisons might result in a type I error being conducted. Significance levels usually only hold for individual tests, so multiple comparisons on the same data simultaneously will inflate the type I error (likelihood of rejecting a null hypothesis when the null hypothesis is true). This should be addressed before the study is conducted. The strategy needs to include consideration of exploratory versus confirmatory research, primary and secondary endpoints, and final vs interim analyses. Planning beforehand will ensure that we are not accidentally increasing our type I error and make our analyses incorrect.

Procedures used for multiple comparison:

- Fisher's protected leas significant difference (LSD) procedure
- Bonferroni method: This method is specifically designed for a certain number of preplanned comparisons. The individual confidence levels are adjusted accordingly by setting individual confidence levels as $\alpha$/# of comparison.s  This is a fairly conservative method, especially with large sample sizes since critical values will be very large and make it unlikely to reject $H_0$, particularly with the large CIs that are produced as a result.
- Tukey-Kramer method yielding the honest significant difference (HSD) for all pairwise comparisons: This method is based on the studentized range distribution and is tailored to the situation of making pairwise comparisons among all means. Only differences betrween group means that are greater than the value of HSD are deemed significantly different.
- Dunnett procedure: Ideal to use to increase power when comparing each treatment group to a single control group for a given study. This increases power since you are performing fewer tests than you would normally do with all pairwise comparisons. The procedure incorporates the dependencies between these comparisons
- Scheffe's method: used for unlimited post hoc testing, meaning that you can explore any or all possible linear contrasts and still have control of the experimental error rate at the chosen $\alpha$. Provides a set of simultaneous 100(1-$\alpha$)% CIs that includes all possible linear contrasts among the factor level means, NOT just the pairwise contrasts.


# 12. Bootstrapping 
Bootstrapping is a statistical estimation procedure coined by Efron (1979), and it is widely used for estimation of complex relationship.

**Rule of Thumb**: Think of bootstrapping instead of the delta method in estimating complex relationships. There are two reasons for the rule: firstly, the delta method is an approximation, which may not be useful for small samples. Second, a sample from a population provides a crude estimate of the population distribution. Utilizing modern computing power, a sampling distribution can be generated and statistics can be estimated from that sampling distribution.

Bootstrapping has become an important tool for estimation since it can be applied to any data set without assuming an underlying distribution.



